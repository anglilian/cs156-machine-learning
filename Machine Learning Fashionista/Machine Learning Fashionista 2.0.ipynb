{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd0ae9e5fb05c4e07405c95453073fad91cde291ec1571923ebb60ab806dff8b1c9",
   "display_name": "Python 3.9.4 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "ae9e5fb05c4e07405c95453073fad91cde291ec1571923ebb60ab806dff8b1c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Assignment 6: Machine Learning Fashionista 2.0\n",
    "## Ang Li-Lian"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the images\n",
    "women_files = glob(r'C:\\Users\\lilia\\Desktop\\CS156\\04-1\\women\\*')\n",
    "men_files = glob(r'C:\\Users\\lilia\\Desktop\\CS156\\04-1\\men\\*')\n",
    "\n",
    "imSize = (64,64)\n",
    "\n",
    "flattened = []\n",
    "# for each image path\n",
    "for gender in [women_files,men_files]:\n",
    "    for path in gender:\n",
    "        # open it as a read file in binary mode\n",
    "        with open(path, 'r+b') as f:\n",
    "            # open it as an image\n",
    "            with Image.open(f) as image:\n",
    "                # resize the image to be more manageable\n",
    "                img = image.resize(imSize)\n",
    "                img = img_to_array(img)\n",
    "                img = img.reshape((img.shape[0], img.shape[1], img.shape[2]))\n",
    "                flattened.append(img)\n",
    "\n",
    "label = [0]*len(women_files) +[1]*len(men_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    np.array(flattened), np.array(label), \n",
    "    test_size=0.2, stratify=np.array(label))"
   ]
  },
  {
   "source": [
    "# Support Vector Machines (SVM)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "There are two parameters we need to select for each SVM: C and gamma. C determines how smooth the function will be or how much it fits the data. The larger the value of C, the more closely the function attempts to fit the data. Gamma determines how much each data point's neighbours influences its classification. The larger the gamma, the smaller the radius of neighbours each point takes into account.\n",
    "\n",
    "To determine the best paramteres for each SVM, we will perform cross-validation and choose the best parameters based on their accuracy."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data for SVM\n",
    "svm_train  = X_train.reshape(-1,64*64*3)\n",
    "svm_test = X_test.reshape(-1,64*64*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv(kernel):\n",
    "    accuracy, g, C = 0, 0,0\n",
    "    for gamma in 10.0**np.arange(-5,2):\n",
    "        for c in 10.0**np.arange(-5,2):\n",
    "            if kernel == \"poly\":\n",
    "                svm = SVC(kernel = kernel, gamma = gamma, C = c, degree = 2)\n",
    "            else:\n",
    "                svm = SVC(kernel = kernel, gamma = gamma, C = c)\n",
    "            scores = cross_val_score(svm, svm_train, y_train, cv=3, scoring=\"accuracy\")\n",
    "            if np.mean(scores)> accuracy:\n",
    "                accuracy, g, C = np.mean(scores), gamma, c\n",
    "    return  accuracy, g , C"
   ]
  },
  {
   "source": [
    "## Linear Kernel"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Accuracy: 0.5888561675925307\nTesting Accuracy: 0.6441351888667992\n"
     ]
    }
   ],
   "source": [
    "linear_accuracy, g, C = cv(\"linear\")\n",
    "linear = SVC(kernel = \"linear\", gamma = g, C = C).fit(svm_train, y_train)\n",
    "linear_pred = linear.predict(svm_test)\n",
    "print(f\"Training Accuracy: {linear_accuracy}\")\n",
    "print(f\"Testing Accuracy: {accuracy_score(y_test, linear_pred)}\")"
   ]
  },
  {
   "source": [
    "## RBF Kernel"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Accuracy: 0.5888561675925307\nTesting Accuracy: 0.5049701789264414\n"
     ]
    }
   ],
   "source": [
    "rbf_accuracy , g, C = cv(\"linear\")\n",
    "rbf = SVC(kernel = \"rbf\", gamma = g, C = C).fit(svm_train, y_train)\n",
    "rbf_pred = rbf.predict(svm_test)\n",
    "print(f\"Training Accuracy: {rbf_accuracy}\")\n",
    "print(f\"Testing Accuracy: {accuracy_score(y_test, rbf_pred)}\")"
   ]
  },
  {
   "source": [
    "## Poly Kernel"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Accuracy: 0.6361458774884917\nTesting Accuracy: 0.6660039761431411\n"
     ]
    }
   ],
   "source": [
    "poly_accuracy, g, C = cv(\"poly\")\n",
    "poly = SVC(kernel = \"poly\", degree = 2,gamma = g, C = C).fit(svm_train, y_train)\n",
    "poly_pred = poly.predict(svm_test)\n",
    "print(f\"Training Accuracy: {poly_accuracy}\")\n",
    "print(f\"Testing Accuracy: {accuracy_score(y_test, poly_pred)}\")"
   ]
  },
  {
   "source": [
    "Each kernel transforms the data into a higher dimensional space to make it easier to seperate out the data into classes. With a linear kernel, it means being able to slice through the data with a hyperplane, an RBF kernel creates different peaks from each centroid and the second degree poly kernel defines a hyperplane using the polynomial function. These kernels become better classifiers when the split in data corresponds to the kernel's shape.  \n",
    "\n",
    "The training accuracy for linear and RBF kernel are similar, but linear does much better than RBF in testing accuracy. It shows that linear is more generalisable with less variance, making it more useful than the RBF kernel which performs much worse on unseen data. This possibly means that the RBF was overfit to the training data which is plausible considering that the RBF is more flexible than the linear and poly kernel. It uses multiple centroids to approximate a function for the data which even though reduces bias, increases its variance. On the other hand, the poly kernel outperforms the linear and RBF kernel in both training and testing accuracy which out of the three kernels makes it the best choice for classifying these images in SVM."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Deep Learning (VGG16)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Loss Function\n",
    "I chose binary cross entropy because it is made for binary classification,increasing the penalty to the function the more wrong predictions made according to a log scale. Compared to a linear loss function, the more confident the algorithm was in a wrong prediction, the more it is penalised compared to only have a small confidence in a wrong prediction. This allows the loss function to converge at a minimum faster."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = preprocess_input(X_train)\n",
    "\n",
    "model = VGG16(weights=\"imagenet\", include_top = False, input_shape=(64,64,3))\n",
    "\n",
    "# Freeze weights\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Set classifier layer\n",
    "x = Flatten(name='flatten')(model.layers[-2].output)\n",
    "output = Dense(1, activation = \"sigmoid\", name = \"prediction\")(x)\n",
    "\n",
    "# Define new model\n",
    "model = Model(inputs=model.inputs, outputs=output)\n",
    "model.compile(loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "201/201 [==============================] - 32s 160ms/step - loss: 0.1181 - accuracy: 0.9856\n",
      "Epoch 2/10\n",
      "201/201 [==============================] - 38s 190ms/step - loss: 0.1545 - accuracy: 0.9811\n",
      "Epoch 3/10\n",
      "201/201 [==============================] - 38s 191ms/step - loss: 0.0617 - accuracy: 0.9890\n",
      "Epoch 4/10\n",
      "201/201 [==============================] - 38s 191ms/step - loss: 0.1067 - accuracy: 0.9841\n",
      "Epoch 5/10\n",
      "201/201 [==============================] - 39s 193ms/step - loss: 0.0814 - accuracy: 0.9866\n",
      "Epoch 6/10\n",
      "201/201 [==============================] - 39s 195ms/step - loss: 0.0725 - accuracy: 0.9930\n",
      "Epoch 7/10\n",
      "201/201 [==============================] - 38s 190ms/step - loss: 0.0244 - accuracy: 0.9965\n",
      "Epoch 8/10\n",
      "201/201 [==============================] - 40s 198ms/step - loss: 0.0952 - accuracy: 0.9895\n",
      "Epoch 9/10\n",
      "201/201 [==============================] - 39s 192ms/step - loss: 0.0879 - accuracy: 0.9871\n",
      "Epoch 10/10\n",
      "201/201 [==============================] - 40s 197ms/step - loss: 0.0542 - accuracy: 0.9915\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18861a8b670>"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "# Train model with images\n",
    "model.fit(images, y_train.reshape((-1,1)), batch_size = 10, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Accuracy: 0.5904572564612326\n"
     ]
    }
   ],
   "source": [
    "VGG16_pred = model.predict(X_test)\n",
    "print(f'Testing Accuracy: {(accuracy_score(y_test, np.round(VGG16_pred)))}')"
   ]
  },
  {
   "source": [
    "# SVM vs Deep Neural Networks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The classification by the neural network clearly outperforms SVM in training accuracy since it makes use of images from ImageNet to help identify salient features of images. However, it only gives a middling performance when it comes to testing accuracy, only more accurate than one out of the three SVM kernels.\n",
    "\n",
    "The training accuracy is so high because it has a more flexible way of classifying images. The SVM pulls images into multi-dimensional space to classify them while being limited by our assumptions on the split in data whereas neural networks classify images based on specific features. This aspect is reflected in the sheer number of weights from ImageNet used to parse through the images.\n",
    "\n",
    "A neural network is the a preferable method on a time constraint because it's training time is significantly shorter since the original weights from ImageNet don't need to be retrained while providing valuable information. Compared to SVM where it needs to be trained and cross-validated across two parameters, the neural network is more efficient. However, if you value accuracy and have the time to wait for it to train, the 5% push that the poly and linear kernel give is worth the few extra hours of training time."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}